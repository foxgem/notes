---
pubDatetime: 2025-03-18T04:19:20Z
title: "A Visual Guide to LLM Agents - by Maarten Grootendorst"
slug: 087636866ba4f0cd3810bd64191383a0
tags:
  - llm agents
  - memory
  - tools
  - planning
  - multi-agent
---

好的，根据您的要求，我将分析提供的文本并输出文章摘要，关键词，概述，分节阅读，相关工具和参考文献。

**语言:** 中文

**关键字:** LLM Agents, Memory, Tools, Planning, Multi-Agent

**概述:**

本文以超过60张定制可视化图表，深入探讨了LLM Agents领域，详细解析了LLM Agents的主要组成部分，并进一步探索了多智能体框架。文章首先介绍了LLM Agents的基本概念，强调了它们通过外部工具、记忆和检索系统来增强LLM能力。随后，文章深入探讨了LLM Agents的三个主要组成部分：记忆（短期和长期）、工具（Toolformer和MCP）和规划（推理、行动和反思）。最后，文章还介绍了多智能体协作的概念，探讨了如何通过多个Agent之间的交互来实现更强大的功能。作者旨在通过本文，帮助读者更好地理解LLM Agents的构建方式，并为GenAI和LLM领域的研究提供有价值的参考。

**分节阅读:**

*   **What Are LLM Agents?**
    *   LLM传统上仅执行下一个token预测，通过连续采样多个token来模拟对话。
    *   LLM的主要缺点是不记得对话，并且在数学计算等任务上表现不佳。
    *   通过外部工具、记忆和检索系统，可以增强LLM的能力，形成“增强型LLM”。

*   **Memory**
    *   LLM本身是无记忆的系统，需要外部机制来模拟记忆功能。
    *   短期记忆可以通过模型的上下文窗口来实现，长期记忆则需要外部向量数据库。
    *   不同类型的记忆可以存储在不同的数据库中，以构建更精细的Agent框架。

*   **Tools**
    *   工具允许LLM与外部环境交互或使用外部应用程序，主要用于获取数据和执行操作。
    *   LLM需要生成符合工具API的文本，通常是JSON格式，以便代码解释器使用。
    *   Toolformer是一种经过训练的模型，可以决定调用哪些API以及如何调用，从而实现工具的使用。

*   **Planning**
    *   规划涉及将任务分解为可操作的步骤，并允许模型迭代地反思过去的行为并更新当前计划。
    *   推理是规划的基础，可以通过微调LLM或特定的提示工程来实现。
    *   ReAct是一种结合推理和行动的技术，通过思想、行动和观察的循环来实现规划。

*   **Multi-Agent Collaboration**
    *   多智能体系统由多个Agent组成，每个Agent都有自己的工具、记忆和规划能力。
    *   多智能体系统通常由专门的Agent组成，每个Agent都配备了自己的工具集，并由主管监督。
    *   多智能体系统的核心是Agent初始化和Agent编排，用于创建和协调各个Agent。

**相关工具:**

*   Code Interpreter: (未提供链接，但通常指可以执行代码的工具)

**参考文献:**

*   Russell, S. J., & Norvig, P. (2016). *Artificial intelligence: a modern approach*. pearson.
*   Sumers, Theodore, et al. "Cognitive architectures for language agents." *Transactions on Machine Learning Research* (2023).
*   Schick, Timo, et al. "Toolformer: Language models can teach themselves to use tools." *Advances in Neural Information Processing Systems* 36 (2023): 68539-68551.
*   Qin, Yujia, et al. "Toolllm: Facilitating large language models to master 16000+ real-world apis." *arXiv preprint arXiv:2307.16789* (2023).
*   Patil, Shishir G., et al. "Gorilla: Large language model connected with massive apis." *Advances in Neural Information Processing Systems* 37 (2024): 126544-126565.
*   "Introducing the Model Context Protocol." *Anthropic*, www.anthropic.com/news/model-context-protocol. Accessed 13 Mar. 2025.
*   Mann, Ben, et al. "Language models are few-shot learners." *arXiv preprint arXiv:2005.14165* 1 (2020): 3.
*   Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." *Advances in neural information processing systems* 35 (2022): 24824-24837.
*   Kojima, Takeshi, et al. "Large language models are zero-shot reasoners." *Advances in neural information processing systems* 35 (2022): 22199-22213.
*   Yao, Shunyu, Zhao, Jeffrey, Yu, Dian, Du, Nan, Shafran, Izhak, Narasimhan, Karthik, and Cao, Yuan. *ReAct: Synergizing Reasoning and Acting in Language Models*. Retrieved from https://par.nsf.gov/biblio/10451467. *International Conference on Learning Representations (ICLR)*.
*   Shinn, Noah, et al. "Reflexion: Language agents with verbal reinforcement learning." *Advances in Neural Information Processing Systems* 36 (2023): 8634-8652.
*   Madaan, Aman, et al. "Self-refine: Iterative refinement with self-feedback." *Advances in Neural Information Processing Systems* 36 (2023): 46534-46594.
*   Park, Joon Sung, et al. "Generative agents: Interactive simulacra of human behavior." *Proceedings of the 36th annual acm symposium on user interface software and technology*. 2023.
*   Wang, Lei, et al. "A survey on large language model based autonomous agents." *Frontiers of Computer Science* 18.6 (2024): 186345.
*   Xi, Zhiheng, et al. "The rise and potential of large language model based agents: A survey." *Science China Information Sciences* 68.2 (2025): 121101.
*   Wu, Qingyun, et al. "Autogen: Enabling next-gen llm applications via multi-agent conversation." *arXiv preprint arXiv:2308.08155* (2023).
*   Hong, Sirui, et al. "Metagpt: Meta programming for multi-agent collaborative framework." *arXiv preprint arXiv:2308.00352* 3.4 (2023): 6.
*   Li, Guohao, et al. "Camel: Communicative agents for" mind" exploration of large language model society." *Advances in Neural Information Processing Systems* 36 (2023): 51991-52008.

**原文链接:** https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-llm-agents

**一致性检查:**

已进行一致性检查，确保整个输出不会出现前后矛盾与原文不符的地方，同时保证段落顺序的一致性。


source: https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-llm-agents