---
pubDatetime: 2025-02-06T23:08:36Z
title: "Understanding Reasoning LLMs - by Sebastian Raschka, PhD"
slug: 9da2bc77ebf95b2c49e21908248205c8
tags:
  - reasoning models
  - llms
  - reinforcement learning
  - supervised finetuning
  - distillation
---

好的，根据您的要求，以下是对文章的分析和输出：

**语言：** 英语

**关键字：** Reasoning Models, LLMs, Reinforcement Learning, Supervised Finetuning, Distillation

**概述：** This article provides a comprehensive overview of reasoning models, a specialized area within the LLM field focused on enhancing LLMs for complex tasks requiring multi-step problem-solving. It defines reasoning models, discusses their advantages and disadvantages, and outlines four main approaches to building and improving them: inference-time scaling, pure reinforcement learning (RL), supervised finetuning (SFT) and reinforcement learning (RL), and pure supervised finetuning (SFT) and distillation. The article also touches on the DeepSeek R1 pipeline as a case study and offers practical advice for developing reasoning models on a limited budget. The author emphasizes the importance of choosing the right type of LLM for the task and highlights the potential of combining different techniques to achieve optimal performance.

**分节阅读：**

*   **How do we define "reasoning model"?**
    *   Reasoning is defined as answering questions requiring complex, multi-step generation.
    *   Reasoning models excel at complex tasks like puzzles and mathematical proofs.
    *   These models often include a "thought" process as part of their response.

*   **When should we use reasoning models?**
    *   Reasoning models are designed for complex tasks like solving puzzles and advanced math problems.
    *   They are not necessary for simpler tasks like summarization or translation.
    *   Using reasoning models for everything can be inefficient and expensive.

*   **A brief look at the DeepSeek training pipeline**
    *   DeepSeek developed three distinct variants: DeepSeek-R1-Zero, DeepSeek-R1, and DeepSeek-R1-Distill.
    *   DeepSeek-R1-Zero was trained using reinforcement learning (RL) without a supervised fine-tuning (SFT) step.
    *   DeepSeek-R1 was further refined with additional SFT stages and further RL training.

*   **The 4 main ways to build and improve reasoning models**
    *   Inference-time scaling improves reasoning capabilities by increasing computational resources during inference.
    *   Pure reinforcement learning (RL) can lead to the emergence of reasoning as a behavior.
    *   Supervised fine-tuning (SFT) plus RL is a common approach for building high-performance reasoning models.
    *   Model distillation involves training smaller models on data generated by larger LLMs.

*   **Thoughts about DeepSeek R1**
    *   The DeepSeek-R1 models are an awesome achievement.
    *   DeepSeek-R1 is more efficient at inference time compared to o1.
    *   It is difficult to compare o1 and DeepSeek-R1 directly because OpenAI has not disclosed much about o1.

*   **Developing reasoning models on a limited budget**
    *   Model distillation offers a more cost-effective alternative.
    *   Smaller, targeted fine-tuning efforts can still yield impressive results at a fraction of the cost.
    *   Journey learning includes incorrect solution paths, allowing the model to learn from mistakes.

**相关工具：**

*   LeetCode compiler: (No direct link provided in the article, use general search)
*   TinyZero: [https://github.com/Jiayi-Pan/TinyZero/](https://github.com/Jiayi-Pan/TinyZero/)

**参考文献：**

*   DeepSeek R1 technical report: [https://arxiv.org/abs/2501.12948](https://arxiv.org/abs/2501.12948)
*   Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters: [https://arxiv.org/abs/2408.03314](https://arxiv.org/abs/2408.03314)
*   Large Language Models are Zero-Shot Reasoners: [https://arxiv.org/abs/2205.11916](https://arxiv.org/abs/2205.11916)
*   LLM Training: RLHF and Its Alternatives: [https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives)
*   Sky-T1: Train your own O1 preview model within $450: [https://novasky-ai.github.io/posts/sky-t1/](https://novasky-ai.github.io/posts/sky-t1/)
*   O1 Replication Journey: A Strategic Progress Report – Part 1: [https://arxiv.org/abs/2410.18982](https://arxiv.org/abs/2410.18982)

**原文链接：** https://magazine.sebastianraschka.com/p/understanding-reasoning-llms


source: https://magazine.sebastianraschka.com/p/understanding-reasoning-llms