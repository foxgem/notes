---
pubDatetime: 2025-01-23T00:51:10Z
title: "Fine-Tune Llama 3.1 Ultra-Efficiently with Unsloth | by Maxime Labonne | Towards Data Science"
slug: fine-tune-llama-3.1-ultra-efficiently-with-unsloth
tags:
  - "Llama 3.1"
  - "Supervised Fine-Tuning"
  - "lora"
  - "qlora"
  - "Unsloth"
---

å¥½çš„ï¼Œè¿™æ˜¯å¯¹æ‚¨æä¾›çš„æ–‡ç« çš„åˆ†æå’Œæ€»ç»“ï¼š

**å…³é”®å­—:**

- Llama 3.1
- Supervised Fine-Tuning (SFT)
- LoRA
- QLoRA
- Unsloth

**æ¦‚è¿°:**

æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨ Unsloth åº“é«˜æ•ˆåœ°å¯¹ Llama 3.1 8B æ¨¡å‹è¿›è¡Œç›‘ç£å¾®è°ƒã€‚æ–‡ç« é¦–å…ˆè§£é‡Šäº†ç›‘ç£å¾®è°ƒçš„æ¦‚å¿µåŠå…¶ä¸æç¤ºå·¥ç¨‹çš„åŒºåˆ«ï¼Œç„¶åæ·±å…¥æ¢è®¨äº†å…¨å¾®è°ƒã€LoRA å’Œ QLoRA ç­‰ä¸»è¦å¾®è°ƒæŠ€æœ¯ï¼Œå¹¶æ¯”è¾ƒäº†å®ƒä»¬çš„ä¼˜ç¼ºç‚¹ã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†ä½¿ç”¨ QLoRA å’Œ Unsloth åº“åœ¨ Google Colab ä¸Šå¾®è°ƒ Llama 3.1 8B æ¨¡å‹çš„å®è·µæ­¥éª¤ï¼ŒåŒ…æ‹¬ç¯å¢ƒé…ç½®ã€æ¨¡å‹åŠ è½½ã€æ•°æ®é›†å‡†å¤‡ã€è®­ç»ƒå‚æ•°è®¾ç½®å’Œæ¨¡å‹ä¿å­˜ã€‚æœ€åï¼Œæ–‡ç« è¿˜æä¾›äº†æ¨¡å‹è¯„ä¼°ã€å¯¹é½ã€é‡åŒ–å’Œéƒ¨ç½²çš„å»ºè®®ï¼Œæ—¨åœ¨å¸®åŠ©è¯»è€…æŒæ¡é«˜æ•ˆå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ã€‚ä½œè€…å¼ºè°ƒäº† Unsloth åº“åœ¨å†…å­˜æ•ˆç‡å’Œè®­ç»ƒé€Ÿåº¦æ–¹é¢çš„ä¼˜åŠ¿ï¼Œå¹¶é¼“åŠ±è¯»è€…å°è¯•å¾®è°ƒåçš„æ¨¡å‹ã€‚

**åˆ†èŠ‚é˜…è¯»:**

- **ğŸ”§ Supervised Fine-Tuning**
  - ç›‘ç£å¾®è°ƒ (SFT) æ˜¯ä¸€ç§é€šè¿‡åœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šé‡æ–°è®­ç»ƒé¢„è®­ç»ƒçš„ LLM æ¥æ”¹è¿›å’Œå®šåˆ¶æ¨¡å‹çš„æ–¹æ³•ã€‚å…¶ä¸»è¦ç›®æ ‡æ˜¯å°†åŸºæœ¬æ¨¡å‹è½¬æ¢ä¸ºå¯ä»¥éµå¾ªæŒ‡ä»¤å’Œå›ç­”é—®é¢˜çš„åŠ©æ‰‹ã€‚SFT è¿˜å¯ä»¥å¢å¼ºæ¨¡å‹çš„æ•´ä½“æ€§èƒ½ï¼Œæ·»åŠ æ–°çŸ¥è¯†æˆ–ä½¿å…¶é€‚åº”ç‰¹å®šä»»åŠ¡å’Œé¢†åŸŸã€‚
- **âš–ï¸ SFT Techniques**
  - å…¨å¾®è°ƒæ˜¯æœ€ç›´æ¥çš„ SFT æŠ€æœ¯ï¼Œå®ƒæ¶‰åŠåœ¨æŒ‡ä»¤æ•°æ®é›†ä¸Šé‡æ–°è®­ç»ƒé¢„è®­ç»ƒæ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼Œä½†éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚LoRA æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯ï¼Œå®ƒå†»ç»“æƒé‡å¹¶åœ¨æ¯ä¸ªç›®æ ‡å±‚å¼•å…¥å°çš„é€‚é…å™¨ï¼Œä»è€Œå¤§å¤§å‡å°‘äº†è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚QLoRA æ˜¯ LoRA çš„æ‰©å±•ï¼Œå¯æä¾›æ›´å¤§çš„å†…å­˜èŠ‚çœï¼Œä½†è®­ç»ƒæ—¶é—´æ›´é•¿ï¼Œé€‚ç”¨äº GPU å†…å­˜å—é™çš„æƒ…å†µã€‚
- **ğŸ¦™ Fine-Tune Llama 3.1 8B**
  - æ–‡ç« ä½¿ç”¨ Unsloth åº“æ¥é«˜æ•ˆå¾®è°ƒ Llama 3.1 8B æ¨¡å‹ï¼Œè¯¥åº“æä¾›äº†æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦å’Œæ›´ä½çš„å†…å­˜ä½¿ç”¨ç‡ã€‚æ–‡ç« ä½¿ç”¨ mlabonne/FineTome-100k æ•°æ®é›†è¿›è¡Œ QLoRA å¾®è°ƒï¼Œå¹¶è¯¦ç»†ä»‹ç»äº†å®‰è£…åº“ã€åŠ è½½æ¨¡å‹ã€é…ç½® LoRA å‚æ•°ã€å‡†å¤‡æ•°æ®é›†å’Œè®¾ç½®è®­ç»ƒå‚æ•°çš„æ­¥éª¤ã€‚è®­ç»ƒå®Œæˆåï¼Œæ–‡ç« è¿˜æ¼”ç¤ºäº†å¦‚ä½•æµ‹è¯•æ¨¡å‹ã€ä¿å­˜æ¨¡å‹ä»¥åŠå°†å…¶è½¬æ¢ä¸º GGUF æ ¼å¼ã€‚

**ç›¸å…³å·¥å…·:**

- **Unsloth:** [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)
- **TRL:** [https://huggingface.co/docs/trl/en/index](https://huggingface.co/docs/trl/en/index)
- **Axolotl:** [https://github.com/OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)
- **LM Studio:** [https://lmstudio.ai/](https://lmstudio.ai/)
- **Ollama:** [https://ollama.com/](https://ollama.com/)
- **text-generation-webui:** [https://github.com/oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui)
- **AutoQuant:** [https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing](https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing)
- **ZeroChat:** [https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC](https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC)

**å‚è€ƒæ–‡çŒ®:**

- **my article about DPO:** [https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html)
- **my article about GGUF and llama.cpp:** [https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html)
- **my article about BF16 format:** [https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html#background-on-floating-point-representation](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html#background-on-floating-point-representation)
- **Rank-Stabilized LoRA:** [https://arxiv.org/abs/2312.03732](https://arxiv.org/abs/2312.03732)
- **LLM Course:** [https://github.com/mlabonne/llm-course](https://github.com/mlabonne/llm-course)
- **LLM Datasets:** [https://github.com/mlabonne/llm-datasets](https://github.com/mlabonne/llm-datasets)
- **Open LLM Leaderboard:** [https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)
- **LLM AutoEval:** [https://github.com/mlabonne/llm-autoeval](https://github.com/mlabonne/llm-autoeval)
- **mlabonne/orpo-dpo-mix-40k:** [https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k)
- **meta-llama/Meta-Llama-3.1-8B:** [https://huggingface.co/meta-llama/Meta-Llama-3.1-8B](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B)
- **unsloth/Meta-Llama-3.1-8B-bnb-4bit:** [https://huggingface.co/unsloth/Meta-Llama-3.1-8B-bnb-4bit](https://huggingface.co/unsloth/Meta-Llama-3.1-8B-bnb-4bit)
- **mlabonne/FineTome-100k:** [https://huggingface.co/datasets/mlabonne/FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k)
- **arcee-ai/The-Tome:** [https://huggingface.co/datasets/arcee-ai/The-Tome](https://huggingface.co/datasets/arcee-ai/The-Tome)
- **arcee-ai/qwen2â€“72b-magpie-en:** [https://huggingface.co/datasets/arcee-ai/qwen2-72b-magpie-en](https://huggingface.co/datasets/arcee-ai/qwen2-72b-magpie-en)
- **HuggingFaceFW/fineweb-edu-classifier:** [https://huggingface.co/HuggingFaceFW/fineweb-edu-classifier](https://huggingface.co/HuggingFaceFW/fineweb-edu-classifier)
- **mlabonne/FineLlama-3.1-8B:** [https://huggingface.co/mlabonne/FineLlama-3.1-8B](https://huggingface.co/mlabonne/FineLlama-3.1-8B)
- **mlabonne/FineLlama-3.1-8B-GGUF:** [https://huggingface.co/mlabonne/FineLlama-3.1-8B-GGUF](https://huggingface.co/mlabonne/FineLlama-3.1-8B-GGUF)

**åŸæ–‡é“¾æ¥:**

[https://towardsdatascience.com/fine-tune-llama-3-1-ultra-efficiently-with-unsloth-7196c7165bab](https://towardsdatascience.com/fine-tune-llama-3-1-ultra-efficiently-with-unsloth-7196c7165bab)

**ä¸€è‡´æ€§æ£€æŸ¥:**

æ‰€æœ‰ä¿¡æ¯å‡ä¸åŸæ–‡ä¸€è‡´ï¼Œæ®µè½é¡ºåºä¹Ÿä¸åŸæ–‡ç›¸åŒã€‚

source: https://towardsdatascience.com/fine-tune-llama-3-1-ultra-efficiently-with-unsloth-7196c7165bab
